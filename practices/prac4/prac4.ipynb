{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241b07b-2e75-44eb-ba94-41b3d668f350",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3cfbe1da6f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a = 'this document is first document'\n",
    "doc_b = 'this document is the second document'\n",
    "\n",
    "bag_of_words_a = doc_a.split(' ')\n",
    "bag_of_words_b = doc_b.split(' ')\n",
    "\n",
    "unique_words_set = set(bag_of_words_a).union(set(bag_of_words_b))\n",
    "print(unique_words_set)\n",
    "\n",
    "# Now create a dictionary of words and their occurence for each document in the corpus (collection of documents).\n",
    "\n",
    "dict_a = dict.fromkeys(unique_words_set, 0)\n",
    "# print(dict_a) # {'this': 0, 'document': 0, 'second': 0, 'is': 0, 'the': 0}\n",
    "\n",
    "for word in bag_of_words_a:\n",
    "    dict_a[word] += 1\n",
    "\n",
    "print(dict_a)\n",
    "# {'this': 1, 'document': 2, 'second': 1, 'is': 1, 'the': 1}\n",
    "\n",
    "# similarly\n",
    "\n",
    "dict_b = dict.fromkeys(unique_words_set, 0)\n",
    "\n",
    "for word in bag_of_words_b:\n",
    "    dict_b[word] += 1\n",
    "\n",
    "print(dict_b)\n",
    "# {'this': 1, 'document': 2, 'second': 1, 'is': 1, 'the': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f3835-64f4-4421-9818-39f356e747c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_term_frequency(word_dictionary, bag_of_words):\n",
    "    term_frequency_dictionary = {}\n",
    "    length_of_bag_of_words = len(bag_of_words)\n",
    "\n",
    "    for word, count in word_dictionary.items():\n",
    "        term_frequency_dictionary[word] = count / float(length_of_bag_of_words)\n",
    "\n",
    "    return term_frequency_dictionary\n",
    "\n",
    "# Implementation\n",
    "\n",
    "print(compute_term_frequency(dict_a, bag_of_words_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39a4f2-8c15-46d5-b711-0dadd816759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_inverse_document_frequency(full_doc_list):\n",
    "    idf_dict = {}\n",
    "    length_of_doc_list = len(full_doc_list)\n",
    "\n",
    "    idf_dict = dict.fromkeys(full_doc_list[0].keys(), 0)\n",
    "    for word, value in idf_dict.items():\n",
    "        idf_dict[word] = math.log(length_of_doc_list / (float(value) + 1))\n",
    "\n",
    "    return idf_dict\n",
    "\n",
    "final_idf_dict = compute_inverse_document_frequency([dict_a, dict_b])\n",
    "print(final_idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bde161-88fe-4681-a312-fb3e789fdf18",
   "metadata": {},
   "outputs": [],
   "source": "# compare two embeddings"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddff3e-7288-4100-a782-69c08f682806",
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abc419-9944-45d7-901b-eac941214237",
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ccf0f0-aecb-4917-a0d0-cb12dd3925cd",
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "37c3c4cf-4643-486a-8878-0379c60c03d5",
   "metadata": {},
   "source": "### Now implementation with sklearn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc81826579aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "corpus_1 = [\n",
    "     'this is sunny morning',\n",
    "     'yesterday was a gloomy morning',\n",
    "     'tomorrow will be rainly morning'\n",
    "]\n",
    "\n",
    "vectorizer.fit(corpus_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9886aee35f0139bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_tf_idf_vectorized = vectorizer.transform(corpus_1)\n",
    "\n",
    "# As the final output of sklearn tf-idf vectorizer is a sparse matrix to save storage space\n",
    "# To visually understand the output better, we need to convert the sparse output matrix to dense matrix with toarray()\n",
    "print(skl_tf_idf_vectorized.toarray())\n",
    "# print(skl_tf_idf_vectorized[0])\n",
    "\n",
    "# As above Even more clear way to visually inspect the output is to convert it to a pandas dataframe\n",
    "# So below I will convert that to a dataframe and then use todense()\n",
    "skl_tfdf_output = skl_tf_idf_vectorized[0]\n",
    "df_tfdf_sklearn = pd.DataFrame(skl_tfdf_output.T.todense(), index=vectorizer.get_feature_names(), columns=['tf-idf'])\n",
    "df_tfdf_sklearn.sort_values(by=[\"tf-idf\"], ascending=True)\n",
    "df_tfdf_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0933c5eb321cff",
   "metadata": {},
   "outputs": [],
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Text classification",
   "id": "ec308748c68f9cd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv', encoding='latin-1')\n",
    "\n",
    "data.head()"
   ],
   "id": "a6ba0c02dbc18251"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)\n",
    "\n",
    "data.columns = ['label', 'text']\n",
    "\n",
    "data.head()"
   ],
   "id": "1888a0dcf5a6575d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data['label'].value_counts(normalize = True).plot.bar()",
   "id": "7bd032df93c82bef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install nltk",
   "id": "acc81a55f655b7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "id": "cc1ab55d32e098c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Task:\n",
    "# lemmatize\n",
    "# build TF-IDF vectors\n",
    "# train-test split\n",
    "# classify\n",
    "# check results"
   ],
   "id": "592ca80a207c4304"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.linear_model import LogisticRegression\n",
   "id": "30fe4376f36ae0f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
